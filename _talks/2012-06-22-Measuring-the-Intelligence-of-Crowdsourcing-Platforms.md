---
title: "Measuring the Intelligence of Crowdsourcing Platforms"
collection: talks
type: "Talk"
permalink: /talks/2012-06-22-Measuring-the-Intelligence-of-Crowdsourcing-Platforms
venue: "NetSci & WebSci Conference, Northwestern University"
date: 2012-06-22
location: "Evanston, Illinois, US"
---
[Video recording](https://www.youtube.com/watch?v=EQ59wGcyquY)

The talk is based on the paper: [Crowd IQ: Measuring the Intelligence of Crowdsourcing Platforms](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/WebSci2012.pdf)

We measure crowdsourcing performance based on a standard IQ questionnaire, and examine Amazon’s Mechanical Turk (AMT) performance under different conditions. These include variations of the payment amount offered, the way incorrect responses affect workers’ reputations, threshold reputation scores of participating AMT workers, and the number of workers per task. We show that crowds composed of workers of high reputation achieve higher performance than low reputation crowds, and the
effect of the amount of payment is non-monotone—both paying too much and too little affects performance. Furthermore, higher performance is achieved when the task
is designed such that incorrect responses can decrease workers’ reputation scores. Using majority vote to aggregate multiple responses to the same task can significantly improve performance, which can be further boosted by dynamically allocating workers to tasks in order to break ties.
